{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests_random_user_agent\n",
    "import requests\n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "import re\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred for: https://www.sec.gov/Archives/edgar/data/320193/a2038036z10q/index.json\n",
      "Error occurred for: https://www.sec.gov/Archives/edgar/data/320193/a10q/index.json\n",
      "Error occurred for: https://www.sec.gov/Archives/edgar/data/789019/d10qa/index.json\n",
      "Error occurred for: https://www.sec.gov/Archives/edgar/data/789019/d10qa/index.json\n",
      "Error occurred for: https://www.sec.gov/Archives/edgar/data/789019/d10q/index.json\n",
      "Error occurred for: https://www.sec.gov/Archives/edgar/data/789019/d10q/index.json\n",
      "Error occurred for: https://www.sec.gov/Archives/edgar/data/789019/d10q/index.json\n",
      "Error occurred for: https://www.sec.gov/Archives/edgar/data/789019/0001032210010002730001/index.json\n",
      "Error occurred for: https://www.sec.gov/Archives/edgar/data/789019/0001032210005000380001/index.json\n"
     ]
    }
   ],
   "source": [
    "class EDGARDataFetcher:\n",
    "\n",
    "    def __init__(self, cik_file_path, filing_type):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the EDGARDataFetcher class.\n",
    "\n",
    "        Args:\n",
    "            cik_file_path (str): Path to the file containing CIK numbers and company names.\n",
    "            filing_type (str): The type of filing to fetch (e.g., '10-k').\n",
    "        \"\"\"\n",
    "        self.cik_file_path = cik_file_path\n",
    "        self.filing_type = filing_type\n",
    "        self.cik_data = {} # Dictionary to store CIK numbers and company names.\n",
    "        self.EDGAR_search_results = {} # Dictionary to store search results.\n",
    "        self.filing_data = {} # Dictionary to store filing data.\n",
    "        self.base_url = r\"https://www.sec.gov\"\n",
    "\n",
    "    def read_cik_data(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Read CIK data from the input file and populate the cik_data dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        with open(self.cik_file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            parts = line.split()\n",
    "\n",
    "            if len(parts) == 2:\n",
    "                word = parts[0]\n",
    "                cik = parts[1]\n",
    "                self.cik_data[word] = cik\n",
    "\n",
    "    def fetch_filings(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Fetch company filings from the SEC's EDGAR database.\n",
    "        \"\"\"\n",
    "        endpoint = r\"https://www.sec.gov/cgi-bin/browse-edgar\"\n",
    "        \n",
    "        for company, cik in self.cik_data.items():\n",
    "            param_dict = {'action':'getcompany',\n",
    "                          'CIK': cik,\n",
    "                          'type': self.filing_type,\n",
    "                          'owner':'exclude',\n",
    "                          'output':'',\n",
    "                          'count':'100'}\n",
    "\n",
    "            response = requests.get(url=endpoint, params=param_dict)\n",
    "            if response.status_code == 200:\n",
    "                self.EDGAR_search_results[company] = response\n",
    "            else:\n",
    "                print('Request Failed')\n",
    "\n",
    "    def get_filing_data(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Extract filing data from the search results and populate the filing_data dictionary.\n",
    "        \"\"\"\n",
    "        \n",
    "        for company, url in self.EDGAR_search_results.items():\n",
    "            soup = BeautifulSoup(url.content, 'html.parser')\n",
    "            doc_table = soup.find_all('table', class_='tableFile2')\n",
    "            master_list = []\n",
    "            \n",
    "            for row in doc_table[0].find_all('tr'):\n",
    "                cols = row.find_all('td')\n",
    "                \n",
    "                if len(cols) != 0:\n",
    "                    filing_type = cols[0].text.strip()\n",
    "                    filing_date = cols[3].text.strip()\n",
    "                    filing_numb = cols[4].text.strip()\n",
    "                    filing_doc_href = cols[1].find('a', {'href':True, 'id':'documentsbutton'})\n",
    "                    filing_int_href = cols[1].find('a', {'href':True, 'id':'interactiveDataBtn'})\n",
    "                    filing_num_href = cols[4].find('a')\n",
    "\n",
    "                    if filing_doc_href != None:\n",
    "                        filing_doc_link = self.base_url + filing_doc_href['href'] \n",
    "                    else:\n",
    "                        filing_doc_link = 'no link'\n",
    "\n",
    "                    if filing_int_href != None:\n",
    "                        filing_int_link = self.base_url + filing_int_href['href'] \n",
    "                    else:\n",
    "                        filing_int_link = 'no link'\n",
    "\n",
    "                    if filing_num_href != None:\n",
    "                        filing_num_link = self.base_url + filing_num_href['href'] \n",
    "                    else:\n",
    "                        filing_num_link = 'no link'\n",
    "\n",
    "                    file_dict = {}\n",
    "                    file_dict['file_type'] = filing_type\n",
    "                    file_dict['file_number'] = filing_numb\n",
    "                    file_dict['file_date'] = filing_date\n",
    "                    file_dict['links'] = {}\n",
    "                    file_dict['links']['documents'] = filing_doc_link\n",
    "\n",
    "                    document_response = requests.get(filing_doc_link)\n",
    "                    document_soup = BeautifulSoup(document_response.content, 'html.parser')\n",
    "                    document_links = document_soup.find_all('a', {'href': lambda href: href.endswith('.txt')})\n",
    "                    document = [document['href'] for document in document_links]\n",
    "                    full_txt_url = \"https://www.sec.gov\" + document[0]\n",
    "                    pattern = r\"/\\d{12,20}/\"\n",
    "                    modified_url = re.sub(pattern, \"/\",full_txt_url)\n",
    "                    document_url = modified_url.replace('-','').replace('.txt', '/index.json')\n",
    "\n",
    "                    try:\n",
    "                        content = requests.get(document_url).json()\n",
    "                        for file in content['directory']['item']:\n",
    "                            if file['name'] == 'FilingSummary.xml':\n",
    "                                xml_summary = self.base_url + content['directory']['name']  + \"/\" + file['name']\n",
    "                                file_dict['links']['documents_xml'] = xml_summary\n",
    "                    except requests.exceptions.RequestException as e:\n",
    "                        print(\"Error occurred for:\", document_url)\n",
    "                        continue\n",
    "\n",
    "                    file_dict['links']['interactive_data'] = filing_int_link\n",
    "                    file_dict['links']['filing_number'] = filing_num_link\n",
    "                    master_list.append(file_dict)\n",
    "\n",
    "                self.filing_data[f\"filing_{company}\"] = master_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred for: https://www.sec.gov/Archives/edgar/data/320193/a2038036z10q/index.json\n",
      "Error occurred for: https://www.sec.gov/Archives/edgar/data/320193/a10q/index.json\n",
      "Error occurred for: https://www.sec.gov/Archives/edgar/data/789019/d10qa/index.json\n",
      "Error occurred for: https://www.sec.gov/Archives/edgar/data/789019/d10qa/index.json\n",
      "Error occurred for: https://www.sec.gov/Archives/edgar/data/789019/d10q/index.json\n",
      "Error occurred for: https://www.sec.gov/Archives/edgar/data/789019/d10q/index.json\n",
      "Error occurred for: https://www.sec.gov/Archives/edgar/data/789019/d10q/index.json\n",
      "Error occurred for: https://www.sec.gov/Archives/edgar/data/789019/0001032210010002730001/index.json\n",
      "Error occurred for: https://www.sec.gov/Archives/edgar/data/789019/0001032210005000380001/index.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cik_file_path = \"cik/liste_cik.txt\"\n",
    "    filing_type = \"10-q\"\n",
    "    \n",
    "    data_fetcher = EDGARDataFetcher(cik_file_path, filing_type)\n",
    "    data_fetcher.read_cik_data()\n",
    "    data_fetcher.fetch_filings()\n",
    "    data_fetcher.get_filing_data()\n",
    "    \n",
    "    filing_data = data_fetcher.filing_data\n",
    "    filing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['filing_aapl', 'filing_msft'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filing_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to filing_documents/filing_aapl.csv\n",
      "Data has been saved to filing_documents/filing_msft.csv\n"
     ]
    }
   ],
   "source": [
    "# Create the \"filing_type\" directory if it doesn't exist\n",
    "folder_name = \"filing_documents\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Iterate through the filing_data dictionary\n",
    "for key, data_list in filing_data.items():\n",
    "    # Specify the name of the CSV file\n",
    "    csv_file = os.path.join(folder_name, f'{key}.csv')\n",
    "\n",
    "    # Create or open the CSV file in write mode\n",
    "    with open(csv_file, mode='w', newline='') as file:\n",
    "        # Define field names for the CSV file\n",
    "        fieldnames = ['file_type', 'file_number', 'file_date', 'documents', 'documents_xml', 'interactive_data', 'filing_number']\n",
    "\n",
    "        # Create a CSV writer and write the header row\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Iterate through each row in the data_list\n",
    "        for row in data_list:\n",
    "            # Extract the links dictionary from the row\n",
    "            links = row.pop('links')\n",
    "\n",
    "            # Update the row with link data\n",
    "            row.update(links)\n",
    "\n",
    "            # Write the row to the CSV file\n",
    "            writer.writerow(row)\n",
    "\n",
    "    # Print a message indicating where the data has been saved\n",
    "    print(f'Data has been saved to {csv_file}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('freqtrade_V1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c76f013e99d6a2ab2e6d4d4ad2bb7f809b2b6bba4e6bbf24e0dae8081eb43f31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
